{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8617a0-1072-4820-8a0e-43302b65746d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 1. Chargement et nettoyage ===\n",
      "\n",
      "=== LSTM Classique ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,608</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m4,608\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,641</span> (18.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,641\u001b[0m (18.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,641</span> (18.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,641\u001b[0m (18.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "--- Entraînement LSTM classique (avec scheduled sampling) ---\n",
      "\n",
      "Epoch 1/10, scheduled_sampling_prob=1.000\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 123.8628 - mae: 10.9116 - val_loss: 7.5527 - val_mae: 2.7190\n",
      "\n",
      "Epoch 2/10, scheduled_sampling_prob=0.950\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 5.2404 - mae: 2.0672 - val_loss: 0.3194 - val_mae: 0.4614\n",
      "\n",
      "Epoch 3/10, scheduled_sampling_prob=0.900\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.7256 - mae: 0.6104 - val_loss: 0.1529 - val_mae: 0.3233\n",
      "\n",
      "Epoch 4/10, scheduled_sampling_prob=0.850\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.5170 - mae: 0.5546 - val_loss: 0.1721 - val_mae: 0.3403\n",
      "\n",
      "Epoch 5/10, scheduled_sampling_prob=0.800\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.5838 - mae: 0.5951 - val_loss: 0.1704 - val_mae: 0.3387\n",
      "\n",
      "Epoch 6/10, scheduled_sampling_prob=0.750\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.5797 - mae: 0.5940 - val_loss: 0.1675 - val_mae: 0.3360\n",
      "\n",
      "Epoch 7/10, scheduled_sampling_prob=0.700\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.5959 - mae: 0.6019 - val_loss: 0.1729 - val_mae: 0.3409\n",
      "\n",
      "Epoch 8/10, scheduled_sampling_prob=0.650\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.5721 - mae: 0.5806 - val_loss: 0.1791 - val_mae: 0.3463\n",
      "\n",
      "Epoch 9/10, scheduled_sampling_prob=0.600\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.5273 - mae: 0.5765 - val_loss: 0.1650 - val_mae: 0.3338\n",
      "\n",
      "Epoch 10/10, scheduled_sampling_prob=0.550\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Layer, RNN\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import pennylane as qml\n",
    "\n",
    "TF_ENABLE_ONEDNN_OPTS=2\n",
    "\n",
    "# ======= 1. Prétraitement et préparation des données =======\n",
    "print(\"\\n=== 1. Chargement et nettoyage ===\")\n",
    "df = pd.read_csv('data/Hamilton.csv', skiprows=1)\n",
    "df = df[pd.to_datetime(df['date'], errors='coerce').notnull()].reset_index(drop=True)\n",
    "df = df.drop(columns=['time'])\n",
    "df = df.rename(columns={'wlvalue': 'water_level', 'flvalue': 'flow'})\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Normalisation et PCA\n",
    "scaler_hydro = MinMaxScaler()\n",
    "hydro_norm = scaler_hydro.fit_transform(df[['water_level', 'flow']])\n",
    "pca = PCA(n_components=1)\n",
    "df['hydro_pca'] = pca.fit_transform(hydro_norm)\n",
    "df['dayofyear'] = df['date'].dt.dayofyear\n",
    "df['sin_doy'] = np.sin(2*np.pi*df['dayofyear']/365)\n",
    "df['cos_doy'] = np.cos(2*np.pi*df['dayofyear']/365)\n",
    "features = ['hydro_pca', 'sin_doy', 'cos_doy']\n",
    "scaler = MinMaxScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "WINDOW = 7\n",
    "X, y = [], []\n",
    "for i in range(WINDOW, len(df)-1):\n",
    "    X.append(df[features].iloc[i-WINDOW:i].values)\n",
    "    y.append(df['water_level'].iloc[i+1])\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "N = len(X)\n",
    "Ntrain = int(0.7*N)\n",
    "Nval = int(0.15*N)\n",
    "X_tr, X_val, X_te = X[:Ntrain], X[Ntrain:Ntrain+Nval], X[Ntrain+Nval:]\n",
    "y_tr, y_val, y_te = y[:Ntrain], y[Ntrain:Ntrain+Nval], y[Ntrain+Nval:]\n",
    "\n",
    "# =============== 2. Scheduled Sampling ===============\n",
    "def scheduled_sampling(X, y, model, scaler, features, prob=0.7):\n",
    "    \"\"\"Modifie la première feature (hydro_pca) des séquences X avec la prédiction du modèle, stochastiquement.\"\"\"\n",
    "    X_mod = X.copy()\n",
    "    for i in range(X.shape[0]):\n",
    "        for t in range(1, X.shape[1]):\n",
    "            if np.random.rand() > prob:\n",
    "                pred = model.predict(X_mod[i:i+1], verbose=0)[0][0]\n",
    "                # Remise à l'échelle normalisée de la prédiction\n",
    "                X_mod[i, t, 0] = scaler.transform(pd.DataFrame([[pred, 0, 0]], columns=features))[0][0]\n",
    "    return X_mod\n",
    "\n",
    "# =============== 3. Entraînement avec schedule ===============\n",
    "def train_with_schedule(model, X, y, X_val, y_val, scaler, features, EPOCHS=10, BATCH_SIZE=16, schedule_decay=0.05, verbose=1):\n",
    "    losses, maes, val_losses, val_maes = [], [], [], []\n",
    "    sampling_prob = 1.0\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}, scheduled_sampling_prob={sampling_prob:.3f}\")\n",
    "        X_mod = scheduled_sampling(X, y, model, scaler, features, prob=sampling_prob)\n",
    "        hist = model.fit(X_mod, y, epochs=1, batch_size=BATCH_SIZE, verbose=verbose, validation_data=(X_val, y_val))\n",
    "        losses.append(hist.history['loss'][0])\n",
    "        maes.append(hist.history['mae'][0])\n",
    "        val_losses.append(hist.history['val_loss'][0])\n",
    "        val_maes.append(hist.history['val_mae'][0])\n",
    "        sampling_prob = max(0.0, sampling_prob - schedule_decay)\n",
    "    return losses, maes, val_losses, val_maes\n",
    "\n",
    "# ======= 4. LSTM Classique =======\n",
    "print(\"\\n=== LSTM Classique ===\")\n",
    "def build_lstm():\n",
    "    model = Sequential([\n",
    "        Input(shape=(WINDOW, len(features))),\n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "lstm = build_lstm()\n",
    "print(lstm.summary())\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "print(\"\\n--- Entraînement LSTM classique (avec scheduled sampling) ---\")\n",
    "losses_lstm, maes_lstm, vallosses_lstm, valmaes_lstm = train_with_schedule(\n",
    "    lstm, X_tr, y_tr, X_val, y_val, scaler, features, EPOCHS=EPOCHS, BATCH_SIZE=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# ======= 5. QLSTM (vrai LSTM quantique) =======\n",
    "print(\"\\n=== QLSTM (vrai LSTM quantique) ===\")\n",
    "N_QUBITS = 4      # 3 features + 1 hidden\n",
    "N_LAYERS = 8\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=N_QUBITS)\n",
    "\n",
    "@qml.qnode(dev, interface='tf')\n",
    "def qru_gate(inputs, weights):\n",
    "    for l in range(N_LAYERS):\n",
    "        for i in range(N_QUBITS):\n",
    "            qml.RY(np.pi * inputs[i], wires=i)\n",
    "            qml.RZ(weights[l, i], wires=i)\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(N_QUBITS)]\n",
    "\n",
    "class QuantumLSTMCell(Layer):\n",
    "    def __init__(self, units=1, n_qubits=N_QUBITS, n_layers=N_LAYERS, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.state_size = [units, units]\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.q_weights = [self.add_weight(\n",
    "            shape=(n_layers, n_qubits), initializer=\"uniform\", trainable=True, name=f'q_weights_{gate}'\n",
    "        ) for gate in ['i','f','c','o']]\n",
    "    def call(self, inputs, states):\n",
    "        h_tm1 = states[0]\n",
    "        c_tm1 = states[1]\n",
    "        concat = tf.concat([inputs, h_tm1], axis=-1)\n",
    "        concat_circ = concat[:, :self.n_qubits]\n",
    "        i = tf.map_fn(lambda x: tf.cast(tf.math.real(tf.convert_to_tensor(qru_gate(x, self.q_weights[0])[0], dtype=tf.float64)), tf.float32), concat_circ, fn_output_signature=tf.float32)\n",
    "        f = tf.map_fn(lambda x: tf.cast(tf.math.real(tf.convert_to_tensor(qru_gate(x, self.q_weights[1])[0], dtype=tf.float64)), tf.float32), concat_circ, fn_output_signature=tf.float32)\n",
    "        c_bar = tf.map_fn(lambda x: tf.cast(tf.math.real(tf.convert_to_tensor(qru_gate(x, self.q_weights[2])[0], dtype=tf.float64)), tf.float32), concat_circ, fn_output_signature=tf.float32)\n",
    "        o = tf.map_fn(lambda x: tf.cast(tf.math.real(tf.convert_to_tensor(qru_gate(x, self.q_weights[3])[0], dtype=tf.float64)), tf.float32), concat_circ, fn_output_signature=tf.float32)\n",
    "        i = tf.sigmoid(i)\n",
    "        f = tf.sigmoid(f)\n",
    "        o = tf.sigmoid(o)\n",
    "        c_bar = tf.tanh(c_bar)\n",
    "        c_t = f * c_tm1[:,0] + i * c_bar\n",
    "        h_t = o * tf.tanh(c_t)\n",
    "        h_t = tf.expand_dims(h_t, -1)\n",
    "        c_t = tf.expand_dims(c_t, -1)\n",
    "        return h_t, [h_t, c_t]\n",
    "\n",
    "inputs = Input(shape=(WINDOW, len(features)))\n",
    "cell = QuantumLSTMCell(units=1, n_qubits=4)\n",
    "rnn = RNN(cell, return_sequences=False)\n",
    "x = rnn(inputs)\n",
    "output = Dense(1, activation='linear')(x)\n",
    "lstm_qru = Model(inputs=inputs, outputs=output)\n",
    "lstm_qru.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(lstm_qru.summary())\n",
    "\n",
    "print(\"\\n--- Entraînement QLSTM (avec scheduled sampling) ---\")\n",
    "losses_lstm_qru, maes_lstm_qru, vallosses_lstm_qru, valmaes_lstm_qru = train_with_schedule(\n",
    "    lstm_qru, X_tr, y_tr, X_val, y_val, scaler, features, EPOCHS=EPOCHS, BATCH_SIZE=4\n",
    ")\n",
    "\n",
    "# ======= 6. Visualisation des courbes d'apprentissage =======\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_lstm, label='Train Loss LSTM')\n",
    "plt.plot(vallosses_lstm, label='Val Loss LSTM')\n",
    "plt.plot(losses_lstm_qru, label='Train Loss QLSTM')\n",
    "plt.plot(vallosses_lstm_qru, label='Val Loss QLSTM')\n",
    "plt.title('Courbes de loss par epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(maes_lstm, label='Train MAE LSTM')\n",
    "plt.plot(valmaes_lstm, label='Val MAE LSTM')\n",
    "plt.plot(maes_lstm_qru, label='Train MAE QLSTM')\n",
    "plt.plot(valmaes_lstm_qru, label='Val MAE QLSTM')\n",
    "plt.title('Courbes de MAE par epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ======= 7. Prédictions finales sur les 30 derniers jours =======\n",
    "y_pred_lstm = lstm.predict(X_te[-30:])\n",
    "y_pred_qru = lstm_qru.predict(X_te[-30:])\n",
    "dates_last = df['date'].iloc[-30:]\n",
    "\n",
    "print(\"\\n--- Données vraies (30 derniers jours):\", y_te[-30:])\n",
    "print(\"--- LSTM predictions:\", y_pred_lstm.flatten())\n",
    "print(\"--- QLSTM predictions:\", y_pred_qru.flatten())\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(dates_last, y_te[-30:], 'o-', label='Vraies valeurs')\n",
    "plt.plot(dates_last, y_pred_lstm, 's--', label='LSTM')\n",
    "plt.plot(dates_last, y_pred_qru, 'd--', label='QLSTM')\n",
    "plt.title('Comparaison vraie vs prédit (30 derniers jours)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Niveau d\\'eau (water_level)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
